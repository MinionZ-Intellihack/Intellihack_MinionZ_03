{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import fitz  # PyMuPDF for extracting text from PDFs\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_cpp import Llama\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = \"AIzaSyBq4gLmdqTbipqUlLOs2ld5uT-ti2Q4EGs\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Define the web scraping tool\n",
    "search = DuckDuckGoSearchRun()\n",
    "# Initialize the Langchain agent with the search tool and Gemini model\n",
    "tools = [search]\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Function to get context from the web based on a prompt\n",
    "def get_context_from_web(prompt):\n",
    "    # Use Langchain to fetch relevant search results from the web\n",
    "    response = agent.run(prompt)\n",
    "    return response\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "embedding_model = embedding_functions.DefaultEmbeddingFunction()\n",
    "vector_db = chromadb.PersistentClient(path=persist_directory)\n",
    "collection = vector_db.get_or_create_collection(name=\"pdf_documents\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text(\"text\") + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def populate_vector_db_from_pdfs(pdf_directory):\n",
    "    \"\"\"Dynamically populates the Chroma vector database from PDFs.\"\"\"\n",
    "    pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "        text_content = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        if text_content:\n",
    "            # Generate embedding\n",
    "            embedding = embedding_model([text_content])[0]\n",
    "\n",
    "            # Add to ChromaDB\n",
    "            collection.add(\n",
    "                ids=[pdf_file],  # Unique identifier (use filename)\n",
    "                embeddings=[embedding],\n",
    "                metadatas=[{\"source\": pdf_file}],\n",
    "                documents=[text_content]\n",
    "            )\n",
    "            print(f\"Added {pdf_file} to the vector database.\")\n",
    "\n",
    "# Usage: Call the function with your PDF directory\n",
    "populate_vector_db_from_pdfs(\"dataset\")\n",
    "\n",
    "def check_vector_db_with_llm(query, top_k=3):\n",
    "    \"\"\"\n",
    "    The LLM itself checks whether the vector database contains sufficient information.\n",
    "    \"\"\"\n",
    "    # Get the collection from the vector database\n",
    "    collection = vector_db.get_collection(name=\"pdf_documents\")\n",
    "\n",
    "    # Perform similarity search\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    # Extract retrieved documents\n",
    "    retrieved_contexts = results.get(\"documents\", [[]])[0]  # Extracts the first list from results\n",
    "    retrieved_context = \"\\n\".join(retrieved_contexts) if retrieved_contexts else \"No relevant context found.\"\n",
    "\n",
    "    # Define the evaluation prompt\n",
    "    eval_prompt = f\"\"\"\n",
    "    You are an AI assistant. Determine if the retrieved context contains enough information to fully answer the question.\n",
    "\n",
    "    Query:\n",
    "    {query}\n",
    "\n",
    "    Retrieved Context:\n",
    "    {retrieved_context}\n",
    "\n",
    "    Answer with \"YES\" if the retrieved context is sufficient, otherwise answer with \"NO\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke LLM and extract response text\n",
    "    decision_message = llm.invoke(eval_prompt)\n",
    "\n",
    "    # Ensure we extract the text correctly\n",
    "    decision = decision_message.content.strip() if hasattr(decision_message, \"content\") else str(decision_message).strip()\n",
    "\n",
    "    return decision, retrieved_context\n",
    "\n",
    "def compress_context(context):\n",
    "    \"\"\"\n",
    "    Compresses the context to a maximum of 1024 characters.\n",
    "    \"\"\"\n",
    "    if len(context) > 1024:\n",
    "        return context[:1021] + \"...\"\n",
    "    return context\n",
    "\n",
    "# Load the GGUF model\n",
    "model_path = \"qwenfinal.gguf\"  # Replace with your GGUF file path\n",
    "qwen = Llama(model_path=model_path, n_ctx=4096) \n",
    "\n",
    "def generate(prompt, model=qwen):\n",
    "    \"\"\"\n",
    "    Generates a response using RAG.\n",
    "    - First checks vector DB for knowledge.\n",
    "    - Uses web search if necessary.\n",
    "    - Combines retrieved knowledge into the final prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Retrieve context from Vector Database\n",
    "    decision, context = check_vector_db_with_llm(prompt)\n",
    "\n",
    "    print(decision)\n",
    "\n",
    "    # Step 2: If vector DB lacks information, use Web Search\n",
    "    if decision.upper() != \"YES\":\n",
    "        context = get_context_from_web(prompt)\n",
    "    \n",
    "    deep_prompt = \"\"\"Below is a question related to AI related researches. Write an answer that appropriately completes the request.\n",
    "\n",
    "    ### Question:\n",
    "    {}\n",
    "\n",
    "    ### Context:\n",
    "    {}\n",
    "\n",
    "    ### Answer:\n",
    "    {}\"\"\"\n",
    "\n",
    "    context = compress_context(context)\n",
    "\n",
    "    # Step 3: Construct Augmented Prompt\n",
    "    deep_prompt = deep_prompt.format(prompt, context, \"\")\n",
    "\n",
    "    response = model(deep_prompt, max_tokens=100, temperature=0.7)\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "response = generate(\"What is Deepseek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deepseek's V3 model is a reasoning model that shows better or equal performance to competitors while achieving it with a fraction of the training and inference cost. The model was downloaded more than ChatGPT, leading to market concerns about AI investments. Deepseek's approach to improving algorithms instead of hardware has made them a disruptor in the industry.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intellihack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
